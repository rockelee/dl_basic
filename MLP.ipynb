{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='mlp.log', level=logging.DEBUG, format=\"%(levelname)s:%(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mnist_loader' from 'mnist_loader.pyc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mnist_loader\n",
    "reload(mnist_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 以字典的形式保存data，\n",
    "# 键包括 X_train, y_train, X_val, y_val\n",
    "num_train = len(training_data)\n",
    "num_val = len(validation_data)\n",
    "num_test = len(test_data)\n",
    "\n",
    "num_dim = training_data[0][0].shape[0]\n",
    "num_classes = training_data[0][1].shape[0]\n",
    "\n",
    "X_train = np.zeros((num_train, num_dim))\n",
    "y_train = np.zeros((num_train, num_classes))\n",
    "for i in range(num_train):\n",
    "    X_train[i, :] = training_data[i][0].ravel()\n",
    "    y_train[i, :] = training_data[i][1]\n",
    "    \n",
    "X_val = np.zeros((num_val, num_dim))\n",
    "y_val = np.zeros((num_val, num_classes))\n",
    "for i in range(num_val):\n",
    "    X_val[i, :] = validation_data[i][0].ravel()\n",
    "    y_val[i, :] = validation_data[i][1]\n",
    "\n",
    "X_test = np.zeros((num_test, num_dim))\n",
    "y_test = np.zeros((num_test, num_classes))\n",
    "for i in range(num_test):\n",
    "    X_test[i, :] = test_data[i][0].ravel()\n",
    "    y_test[i, :] = test_data[i][1]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)\n",
    "y_val = y_val.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "data = {}\n",
    "data['X_train'] = X_train\n",
    "data['y_train'] = y_train\n",
    "data['X_val'] = X_val\n",
    "data['y_val'] = y_val\n",
    "data['X_test'] = X_test\n",
    "data['y_test'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000, 10) (10000, 784) (10000, 10)\n",
      "float32 float32 float32 float32\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape, y_train.shape, X_val.shape, y_val.shape\n",
    "print X_train.dtype, y_train.dtype, X_val.dtype, y_val.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  50 158 252\n",
      "  158  49   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  47 237 251 251\n",
      "  251 236   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  53 226 252 251 238\n",
      "  232 251  56   5   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   9  59 223 251 252 251 201\n",
      "   83 251 252 121   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 162 251 251 251 252 251 251\n",
      "   95 188 252 166   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  50 237 252 252 189 113 252 227\n",
      "   46  78 254 167   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  47 237 251 251 178  11  74 120  20\n",
      "    0   0 252 242  49   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  37 164 252 232 207  83   0   0   0   0\n",
      "    0   0 252 251 164   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   6 177 251 239  70  18  27   0   0   0   0\n",
      "    0   0 252 251 194   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  56 251 251  62   0   0   0   0   0   0   0\n",
      "    0   0 252 251 194   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 197 252 189   0   0   0   0   0   0   0   0\n",
      "    0   0 254 252 195   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  75 245 251 111   0   0   0   0   0   0   0   0\n",
      "    0   0 252 251 147   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  84 251 229  24   0   0   0   0   0   0   0   0\n",
      "    6 134 252 185  11   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  84 251 222   0   0   0   0   0   0   0   0   6\n",
      "  130 251 224  70   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  84 251 144   0   0   0   0   0   0   0  47 164\n",
      "  251 172   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 224   0   0   0   0   0   0 113 237 252\n",
      "  161   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  84 251 248 145  47  28  84 177 224 252 222 166\n",
      "   55   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  84 251 251 251 228 214 251 251 251 195 129   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  27 198 251 251 252 251 251 232 144   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  24 127 251 252 251 140  36   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print (X_train[1].reshape((28,-1))*255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \"\"\"\n",
    "    multi layer perceptron\n",
    "    -------------------------\n",
    "    input_dim : \n",
    "    hidden_layers : \n",
    "    num_classes : \n",
    "    reg :           regularization penalty\n",
    "    keep_prob :     if False, do not use dropout, otherwise it means keep probability\n",
    "    use_batchnorm : if False, do not use bn, otherwise it means decay, usually larger than 0.9\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers, num_classes, reg=0.0, keep_prob=False, use_batchnorm=False):    \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_hidden = len(hidden_layers)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.X = tf.placeholder(dtype=tf.float32, shape=[None, input_dim], name='input_x')\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=[None, num_classes], name='output_y')\n",
    "        \n",
    "        self.reg = reg\n",
    "        self.l2_penalty = tf.constant(0.0)\n",
    "        \n",
    "        self.drop_keep_rate = keep_prob\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        \n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.build(\"MLP\")\n",
    "        pass\n",
    "    \n",
    "    def build(self, prefix):\n",
    "        if self.num_hidden != 0:\n",
    "            layer_nodes = [self.input_dim] + self.hidden_layers + [self.num_classes]\n",
    "        else:\n",
    "            layer_nodes = [self.input_dim, self.num_classes]\n",
    "        \n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.all_l2_loss = []\n",
    "        \n",
    "        # hidden layer\n",
    "        incoming = self.X\n",
    "        for i in range(self.num_hidden):\n",
    "            if self.use_batchnorm:\n",
    "                incoming, l2_loss = self.affine_bn_relu(incoming, layer_nodes[i], layer_nodes[i+1], \\\n",
    "                                                 layer_name=prefix+\"_hid_\"+str(i+1), act_fn=tf.nn.relu) \n",
    "                self.all_l2_loss.append(l2_loss)\n",
    "            else :\n",
    "                incoming, l2_loss = self.affine_relu(incoming, layer_nodes[i], layer_nodes[i+1], \\\n",
    "                                                       layer_name=prefix+\"_hid_\"+str(i+1), act_fn=tf.nn.relu) \n",
    "                self.all_l2_loss.append(l2_loss)\n",
    "            \n",
    "            if self.drop_keep_rate:\n",
    "                incoming = self.drop_layer(incoming, layer_name=prefix+'_drop_'+str(i+1))\n",
    "        \n",
    "        # output layer\n",
    "        self.output, l2_loss = self.affine_relu(incoming, layer_nodes[-2], layer_nodes[-1], \\\n",
    "                                                layer_name=prefix+\"_output\", act_fn=tf.nn.relu)\n",
    "        self.all_l2_loss.append(l2_loss)\n",
    "        \n",
    "        # loss\n",
    "        total_entropy_loss = tf.losses.softmax_cross_entropy(logits=self.output, onehot_labels=self.y)\n",
    "        self.mean_entropy_loss = tf.reduce_mean(total_entropy_loss, name='mean_entropy_loss')\n",
    "        \n",
    "        for l2 in self.all_l2_loss:\n",
    "            self.l2_penalty += l2\n",
    "        self.loss = self.mean_entropy_loss + self.l2_penalty * self.reg\n",
    "        \n",
    "        # accuracy\n",
    "        self.predict_score = tf.argmax(self.output, 1, name='predict')\n",
    "        self.ground_truth = tf.argmax(self.y, 1, name='ground_truth')\n",
    "        corrent_prediction = tf.equal(self.predict_score, self.ground_truth)\n",
    "        self.accuracy = tf.reduce_mean( tf.cast(corrent_prediction, tf.float32), name='accuracy')\n",
    "    \n",
    "    def affine_relu(self, in_tensor, in_dim, out_dim, layer_name, act_fn=tf.nn.relu):\n",
    "        with tf.name_scope(layer_name):\n",
    "            init_w = tf.truncated_normal(mean=0, stddev=1./np.sqrt(in_dim), shape=[in_dim, out_dim])\n",
    "            w = tf.Variable(init_w, name='weights')\n",
    "            b = tf.Variable(tf.zeros([out_dim]), name='bias')\n",
    "            \n",
    "            out_affine = tf.nn.bias_add(tf.matmul(in_tensor, w), b, name=\"out_affine\")\n",
    "            out_act = act_fn(out_affine, name=\"out_act\")\n",
    "            \n",
    "            print w.name, b.name, out_affine.name, out_act.name\n",
    "            \n",
    "            l2_loss = tf.nn.l2_loss(w)\n",
    "            self.W.append(w)\n",
    "            self.b.append(b)\n",
    "            \n",
    "        return out_act, l2_loss\n",
    "    \n",
    "    def affine_bn_relu(self, in_tensor, in_dim, out_dim, layer_name, act_fn=tf.nn.relu):\n",
    "        with tf.name_scope(layer_name):\n",
    "            init_w = tf.truncated_normal(mean=0, stddev=1./np.sqrt(in_dim), shape=[in_dim, out_dim])\n",
    "            w = tf.Variable(init_w, name='weights')\n",
    "            b = tf.Variable(tf.zeros([out_dim]), name='bias')\n",
    "            \n",
    "            out_affine = tf.nn.bias_add(tf.matmul(in_tensor, w), b, name='out_affine')\n",
    "            out_bn = self.batch_norm(out_affine, in_dim, out_dim, layer_name)\n",
    "            out_act = act_fn(out_bn, name='out_act')\n",
    "            \n",
    "            print w.name, b.name, out_affine.name, out_bn.name, out_act.name\n",
    "            \n",
    "            l2_loss = tf.nn.l2_loss(w)\n",
    "            self.W.append(w)\n",
    "            self.b.append(b)\n",
    "\n",
    "        return out_act, l2_loss\n",
    "    \n",
    "    def batch_norm(self, in_tensor, in_dim, out_dim, layer_name):\n",
    "        epsilon = 1e-4\n",
    "        running_mean = tf.Variable(tf.zeros([out_dim]), trainable=False, name='running_mean')\n",
    "        running_var = tf.Variable(tf.ones([out_dim]), trainable=False, name='running_var')\n",
    "            \n",
    "        beta = tf.Variable(tf.zeros([out_dim]), name='beta')\n",
    "        gamma = tf.Variable(tf.ones([out_dim]), name='gamma')\n",
    "        \n",
    "        print running_mean.name, running_var.name, beta.name, gamma.name\n",
    "        mean, var = tf.nn.moments(in_tensor, [0])\n",
    "        def train_fn():\n",
    "            train_mean = tf.assign(running_mean, running_mean * self.use_batchnorm + mean * (1.0 - self.use_batchnorm))\n",
    "            train_var = tf.assign(running_var, running_var * self.use_batchnorm + var * (1.0 - self.use_batchnorm))\n",
    "            \n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                return tf.nn.batch_normalization(in_tensor, mean, var, beta, gamma, epsilon)\n",
    "        \n",
    "        def test_fn():\n",
    "            return tf.nn.batch_normalization(in_tensor, running_mean, running_var, beta, gamma, epsilon)\n",
    "        \n",
    "        return tf.cond(self.is_train, train_fn, test_fn, name='bn')\n",
    "\n",
    "\n",
    "    def drop_layer(self, in_tensor, layer_name):\n",
    "        with tf.name_scope(layer_name):\n",
    "            out_drop = tf.nn.dropout(in_tensor, self.drop_keep_rate, name='out_drop')\n",
    "            print out_drop.name\n",
    "        return out_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sovler(object):\n",
    "    def __init__(self, session, model, data, **kwargs):\n",
    "        self.sess = session\n",
    "        self.model = model\n",
    "        \n",
    "        self.X_train = data['X_train']\n",
    "        self.y_train = data['y_train']\n",
    "        self.X_val = data['X_val']\n",
    "        self.y_val = data['y_val']\n",
    "        \n",
    "        self.num_train = self.X_train.shape[0]\n",
    "        self.num_val = self.X_val.shape[0]\n",
    "        \n",
    "        self.num_epochs = kwargs.pop('num_epochs', 10)\n",
    "        self.batch_size = kwargs.pop('batch_size', 100)\n",
    "        self.verbose = kwargs.pop('verbose', True)\n",
    "        self.print_every = kwargs.pop('print_every', 10)\n",
    "        \n",
    "        # train Operation\n",
    "        # early stopping\n",
    "        self.best_validation_acc = -np.inf\n",
    "        self.max_epochs_no_best = 2\n",
    "        \n",
    "        # sgd\n",
    "#         self.optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "        # sgd + momentum\n",
    "        self.optimizer = tf.train.MomentumOptimizer(0.5, momentum=0.6)\n",
    "        # rmsprop\n",
    "#         self.optimizer = tf.train.RMSPropOptimizer(0.5, decay=0.9)\n",
    "        # adam\n",
    "#         self.optimizer = tf.train.AdamOptimizer(0.5)\n",
    "        \n",
    "        self.train_step = self.optimizer.minimize(self.model.loss)        \n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        mode = 'train'\n",
    "        \n",
    "        num_iter_per_epoch = math.ceil(self.num_train * 1. / self.batch_size)\n",
    "        num_iters = self.num_epochs * num_iter_per_epoch\n",
    "        print (\"num_epochs: {0}, num_iter_per_epoch: {1}, num_iters: {2}\" \\\n",
    "              .format(self.num_epochs, num_iter_per_epoch, num_iters))        \n",
    "        iter_cnt = 0\n",
    "        train_indices = np.arange(self.num_train)\n",
    "        \n",
    "        #early stopping\n",
    "        no_better_validation_step = 0\n",
    "        losses = []\n",
    "        for i in range(self.num_epochs):\n",
    "            np.random.shuffle(train_indices)\n",
    "            for j in range(int(num_iter_per_epoch)):\n",
    "                start_idx = j * self.batch_size\n",
    "                end_idx = (j+1) * self.batch_size\n",
    "                idx = train_indices[start_idx : end_idx]\n",
    "                actual_batch_size = idx.shape[0]\n",
    "                \n",
    "                xt = self.X_train[idx]\n",
    "                yt = self.y_train[idx]\n",
    "                \n",
    "                feed_dict = {self.model.X : xt, self.model.y : yt, self.model.is_train : mode == 'train'}\n",
    "                fetchs = [self.model.loss, self.model.accuracy, self.model.predict_score, self.train_step]\n",
    "                \n",
    "                loss, acc, _, _ = self.sess.run(fetchs, feed_dict=feed_dict)\n",
    "                \n",
    "                losses.append(losses)\n",
    "                \n",
    "                if self.verbose and iter_cnt % self.print_every == 0:\n",
    "                    print(\"Iteration({0} / {1}), actual_batch_size {2}, batch_loss {3}, acc {4}\" \\\n",
    "                           .format(iter_cnt, num_iters, actual_batch_size, loss, acc))\n",
    "                iter_cnt += 1\n",
    "        \n",
    "            # test阶段是不做validation的\n",
    "            # 在一轮 epoch 完成以后验证, 并判断early stopping\n",
    "            # 注意验证时，应该使得 is_train 为 False\n",
    "            if mode == 'train':\n",
    "                no_better_validation_step = self.validation(no_better_validation_step, i+1, mode)\n",
    "                if no_better_validation_step > self.max_epochs_no_best:\n",
    "                    break\n",
    "\n",
    "    def validation(self, no_better_validation_step, th_epoch, mode):\n",
    "        feed_dict = {self.model.X : self.X_val, self.model.y : self.y_val, self.model.is_train : mode == 'train'}\n",
    "        fetchs = [self.model.loss, self.model.accuracy]\n",
    "        val_loss, val_acc = self.sess.run(fetchs, feed_dict=feed_dict)\n",
    "        print(\"validation, Epochs({0} / {1}), val_loss {2}, val_acc {3}\"\\\n",
    "                  .format(th_epoch, self.num_epochs, val_loss, val_acc))\n",
    "        if val_acc > self.best_validation_acc:\n",
    "            self.best_validation_acc = val_acc\n",
    "            no_better_validation_step = 0\n",
    "        else:\n",
    "            no_better_validation_step+=1\n",
    "        return no_better_validation_step\n",
    "\n",
    "    def test(self, X, y):\n",
    "        mode = 'test'\n",
    "        \n",
    "        feed_dict = {self.model.X : X, self.model.y : y, self.model.is_train : False}\n",
    "        fetchs = [self.model.predict_score, self.model.accuracy]\n",
    "        predict_score, acc = self.sess.run(fetchs, feed_dict=feed_dict)\n",
    "        \n",
    "        print \"test acc {0}\".format(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_hid_1/running_mean:0 MLP_hid_1/running_var:0 MLP_hid_1/beta:0 MLP_hid_1/gamma:0\n",
      "MLP_hid_1/weights:0 MLP_hid_1/bias:0 MLP_hid_1/out_affine:0 MLP_hid_1/bn/Merge:0 MLP_hid_1/out_act:0\n",
      "MLP_hid_2/running_mean:0 MLP_hid_2/running_var:0 MLP_hid_2/beta:0 MLP_hid_2/gamma:0\n",
      "MLP_hid_2/weights:0 MLP_hid_2/bias:0 MLP_hid_2/out_affine:0 MLP_hid_2/bn/Merge:0 MLP_hid_2/out_act:0\n",
      "MLP_output/weights:0 MLP_output/bias:0 MLP_output/out_affine:0 MLP_output/out_act:0\n",
      "num_epochs: 2, num_iter_per_epoch: 500.0, num_iters: 1000.0\n",
      "Iteration(0 / 1000.0), actual_batch_size 100, batch_loss 2.54713273048, acc 0.159999996424\n",
      "Iteration(100 / 1000.0), actual_batch_size 100, batch_loss 0.669258236885, acc 0.870000004768\n",
      "Iteration(200 / 1000.0), actual_batch_size 100, batch_loss 0.537691473961, acc 0.930000007153\n",
      "Iteration(300 / 1000.0), actual_batch_size 100, batch_loss 0.731281638145, acc 0.850000023842\n",
      "Iteration(400 / 1000.0), actual_batch_size 100, batch_loss 0.657475948334, acc 0.860000014305\n",
      "validation, Epochs(1 / 2), val_loss 0.524679362774, val_acc 0.921500086784\n",
      "Iteration(500 / 1000.0), actual_batch_size 100, batch_loss 0.498081445694, acc 0.910000026226\n",
      "Iteration(600 / 1000.0), actual_batch_size 100, batch_loss 0.705492436886, acc 0.839999973774\n",
      "Iteration(700 / 1000.0), actual_batch_size 100, batch_loss 0.553771853447, acc 0.920000016689\n",
      "Iteration(800 / 1000.0), actual_batch_size 100, batch_loss 0.561501502991, acc 0.909999966621\n",
      "Iteration(900 / 1000.0), actual_batch_size 100, batch_loss 0.708291053772, acc 0.849999964237\n",
      "validation, Epochs(2 / 2), val_loss 0.562901496887, val_acc 0.915100097656\n",
      "test acc 0.896100103855\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "\n",
    "num_dim = 784\n",
    "hidden_layer = [30]\n",
    "num_classes = 10\n",
    "model = MLP(784, [30, 30], 10, reg=1e-2, keep_prob=False, use_batchnorm=0.9)\n",
    "# logger.info(\"Data Dim:\" + str([num_dim] + hidden_layer + [num_classes]))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    solver = Sovler(sess, model, data, num_epochs=2, batch_size=100, verbose=True, print_every=100)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    solver.run()\n",
    "    \n",
    "    solver.test(data['X_test'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
