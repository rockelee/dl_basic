{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"./logger.log\", level=logging.DEBUG)\n",
    "logging.debug(\"fuckyou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"fuckhe1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lilinhan/workspace/software/anaconda/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  # Import data\n",
    "  mnist = input_data.read_data_sets(data_dir,\n",
    "                                    one_hot=True,\n",
    "                                    fake_data=fake_data,\n",
    "                                    source_url=\"http://yann.lecun.com/exdb/mnist/\",\n",
    "                                    )\n",
    "\n",
    "  sess = tf.InteractiveSession()\n",
    "  # Create a multilayer model.\n",
    "\n",
    "  # Input placeholders\n",
    "  with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "\n",
    "  with tf.name_scope('input_reshape'):\n",
    "    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "  # We can't initialize these variables to 0 - the network will get stuck.\n",
    "  def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "  def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "  def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "      mean = tf.reduce_mean(var)\n",
    "      tf.summary.scalar('mean', mean)\n",
    "      with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "      tf.summary.scalar('stddev', stddev)\n",
    "      tf.summary.scalar('max', tf.reduce_max(var))\n",
    "      tf.summary.scalar('min', tf.reduce_min(var))\n",
    "      tf.summary.histogram('histogram', var)\n",
    "\n",
    "  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "    with tf.name_scope(layer_name):\n",
    "      # This Variable will hold the state of the weights for the layer\n",
    "      with tf.name_scope('weights'):\n",
    "        weights = weight_variable([input_dim, output_dim])\n",
    "        variable_summaries(weights)\n",
    "      with tf.name_scope('biases'):\n",
    "        biases = bias_variable([output_dim])\n",
    "        variable_summaries(biases)\n",
    "      with tf.name_scope('Wx_plus_b'):\n",
    "        preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "        tf.summary.histogram('pre_activations', preactivate)\n",
    "      activations = act(preactivate, name='activation')\n",
    "      tf.summary.histogram('activations', activations)\n",
    "      return activations\n",
    "\n",
    "  hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "  with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "  # Do not apply softmax activation yet, see below.\n",
    "  y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "  with tf.name_scope('cross_entropy'):\n",
    "    # The raw formulation of cross-entropy,\n",
    "    #\n",
    "    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "    #                               reduction_indices=[1]))\n",
    "    #\n",
    "    # can be numerically unstable.\n",
    "    #\n",
    "    # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "    # raw outputs of the nn_layer above, and then average across\n",
    "    # the batch.\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "    with tf.name_scope('total'):\n",
    "      cross_entropy = tf.reduce_mean(diff)\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "        cross_entropy)\n",
    "\n",
    "  with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "  # Merge all the summaries and write them out to\n",
    "  # /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n",
    "  merged = tf.summary.merge_all()\n",
    "  train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "  test_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  # Train the model, and also write summaries.\n",
    "  # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "  # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "  def feed_dict(train):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train or fake_data:\n",
    "      xs, ys = mnist.train.next_batch(100, fake_data=fake_data)\n",
    "      k = dropout\n",
    "    else:\n",
    "      xs, ys = mnist.test.images, mnist.test.labels\n",
    "      k = 1.0\n",
    "    return {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "  for i in range(max_steps):\n",
    "    if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "      test_writer.add_summary(summary, i)\n",
    "      print('Accuracy at step %s: %s' % (i, acc))\n",
    "    else:  # Record train set summaries, and train\n",
    "      if i % 100 == 99:  # Record execution stats\n",
    "#         run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "#         run_metadata = tf.RunMetadata()\n",
    "#         summary, _ = sess.run([merged, train_step],\n",
    "#                               feed_dict=feed_dict(True),\n",
    "#                               options=run_options,\n",
    "#                               run_metadata=run_metadata)\n",
    "#         train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "#         train_writer.add_summary(summary, i)\n",
    "#         print('Adding run metadata for', i)\n",
    "        pass\n",
    "      else:  # Record a summary\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "        train_writer.add_summary(summary, i)\n",
    "  train_writer.close()\n",
    "  test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_data = False\n",
    "max_steps = 1000\n",
    "learning_rate = 0.001\n",
    "dropout = 0.9\n",
    "data_dir = \"./mnist_data\"\n",
    "log_dir = \"./tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy at step 0: 0.1093\n",
      "Accuracy at step 10: 0.6817\n",
      "Accuracy at step 20: 0.8137\n",
      "Accuracy at step 30: 0.8614\n",
      "Accuracy at step 40: 0.8797\n",
      "Accuracy at step 50: 0.8911\n",
      "Accuracy at step 60: 0.8945\n",
      "Accuracy at step 70: 0.9054\n",
      "Accuracy at step 80: 0.9028\n",
      "Accuracy at step 90: 0.9092\n",
      "Accuracy at step 100: 0.9114\n",
      "Accuracy at step 110: 0.9156\n",
      "Accuracy at step 120: 0.9212\n",
      "Accuracy at step 130: 0.9241\n",
      "Accuracy at step 140: 0.9246\n",
      "Accuracy at step 150: 0.9241\n",
      "Accuracy at step 160: 0.9291\n",
      "Accuracy at step 170: 0.9311\n",
      "Accuracy at step 180: 0.9329\n",
      "Accuracy at step 190: 0.9315\n",
      "Accuracy at step 200: 0.9352\n",
      "Accuracy at step 210: 0.9372\n",
      "Accuracy at step 220: 0.9381\n",
      "Accuracy at step 230: 0.9382\n",
      "Accuracy at step 240: 0.9369\n",
      "Accuracy at step 250: 0.9425\n",
      "Accuracy at step 260: 0.9404\n",
      "Accuracy at step 270: 0.9408\n",
      "Accuracy at step 280: 0.9415\n",
      "Accuracy at step 290: 0.9443\n",
      "Accuracy at step 300: 0.9446\n",
      "Accuracy at step 310: 0.9435\n",
      "Accuracy at step 320: 0.9453\n",
      "Accuracy at step 330: 0.9491\n",
      "Accuracy at step 340: 0.9484\n",
      "Accuracy at step 350: 0.9459\n",
      "Accuracy at step 360: 0.9468\n",
      "Accuracy at step 370: 0.9497\n",
      "Accuracy at step 380: 0.9489\n",
      "Accuracy at step 390: 0.9504\n",
      "Accuracy at step 400: 0.9514\n",
      "Accuracy at step 410: 0.9533\n",
      "Accuracy at step 420: 0.951\n",
      "Accuracy at step 430: 0.9469\n",
      "Accuracy at step 440: 0.9527\n",
      "Accuracy at step 450: 0.9525\n",
      "Accuracy at step 460: 0.9544\n",
      "Accuracy at step 470: 0.9539\n",
      "Accuracy at step 480: 0.9531\n",
      "Accuracy at step 490: 0.9534\n",
      "Accuracy at step 500: 0.9567\n",
      "Accuracy at step 510: 0.9552\n",
      "Accuracy at step 520: 0.9574\n",
      "Accuracy at step 530: 0.9577\n",
      "Accuracy at step 540: 0.9559\n",
      "Accuracy at step 550: 0.9592\n",
      "Accuracy at step 560: 0.9563\n",
      "Accuracy at step 570: 0.9597\n",
      "Accuracy at step 580: 0.9579\n",
      "Accuracy at step 590: 0.9579\n",
      "Accuracy at step 600: 0.9578\n",
      "Accuracy at step 610: 0.9588\n",
      "Accuracy at step 620: 0.9589\n",
      "Accuracy at step 630: 0.9615\n",
      "Accuracy at step 640: 0.9627\n",
      "Accuracy at step 650: 0.962\n",
      "Accuracy at step 660: 0.96\n",
      "Accuracy at step 670: 0.9614\n",
      "Accuracy at step 680: 0.9602\n",
      "Accuracy at step 690: 0.9612\n",
      "Accuracy at step 700: 0.9615\n",
      "Accuracy at step 710: 0.9608\n",
      "Accuracy at step 720: 0.9588\n",
      "Accuracy at step 730: 0.9591\n",
      "Accuracy at step 740: 0.9589\n",
      "Accuracy at step 750: 0.9603\n",
      "Accuracy at step 760: 0.96\n",
      "Accuracy at step 770: 0.9631\n",
      "Accuracy at step 780: 0.9647\n",
      "Accuracy at step 790: 0.9611\n",
      "Accuracy at step 800: 0.963\n",
      "Accuracy at step 810: 0.9625\n",
      "Accuracy at step 820: 0.9606\n",
      "Accuracy at step 830: 0.9646\n",
      "Accuracy at step 840: 0.9648\n",
      "Accuracy at step 850: 0.9639\n",
      "Accuracy at step 860: 0.964\n",
      "Accuracy at step 870: 0.9663\n",
      "Accuracy at step 880: 0.9662\n",
      "Accuracy at step 890: 0.9645\n",
      "Accuracy at step 900: 0.9646\n",
      "Accuracy at step 910: 0.9642\n",
      "Accuracy at step 920: 0.9642\n",
      "Accuracy at step 930: 0.9642\n",
      "Accuracy at step 940: 0.9676\n",
      "Accuracy at step 950: 0.9658\n",
      "Accuracy at step 960: 0.9669\n",
      "Accuracy at step 970: 0.9677\n",
      "Accuracy at step 980: 0.9678\n",
      "Accuracy at step 990: 0.9656\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lilinhan/workspace/software/anaconda/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "def main():\n",
    "  # If the training and test sets aren't stored locally, download them.\n",
    "    if not os.path.exists(IRIS_TRAINING):\n",
    "        raw = urlopen(IRIS_TRAINING_URL).read()\n",
    "        with open(IRIS_TRAINING, \"wb\") as f:\n",
    "            f.write(raw)\n",
    "\n",
    "    if not os.path.exists(IRIS_TEST):\n",
    "        raw = urlopen(IRIS_TEST_URL).read()\n",
    "        with open(IRIS_TEST, \"wb\") as f:\n",
    "            f.write(raw)\n",
    "\n",
    "    # Load datasets.\n",
    "    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=IRIS_TRAINING,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=IRIS_TEST,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "    return training_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set, test_set = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify that all features have real-value data\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='x', shape=(4,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9cfbe13bd0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/iris_model', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                        hidden_units=[10, 20, 10],\n",
    "                                        n_classes=3,\n",
    "                                        model_dir=\"/tmp/iris_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the training inputs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(training_set.data)},\n",
    "    y=np.array(training_set.target),\n",
    "    num_epochs=None,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 186.71275, step = 1\n",
      "INFO:tensorflow:global_step/sec: 288.929\n",
      "INFO:tensorflow:loss = 16.507725, step = 101 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.127\n",
      "INFO:tensorflow:loss = 11.787524, step = 201 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.714\n",
      "INFO:tensorflow:loss = 6.909089, step = 301 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.991\n",
      "INFO:tensorflow:loss = 10.808114, step = 401 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.131\n",
      "INFO:tensorflow:loss = 6.6032157, step = 501 (0.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 404.037\n",
      "INFO:tensorflow:loss = 11.972122, step = 601 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.09\n",
      "INFO:tensorflow:loss = 4.5720434, step = 701 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.475\n",
      "INFO:tensorflow:loss = 3.5540428, step = 801 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.215\n",
      "INFO:tensorflow:loss = 7.845985, step = 901 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.111\n",
      "INFO:tensorflow:loss = 2.7546103, step = 1001 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 402.424\n",
      "INFO:tensorflow:loss = 8.015123, step = 1101 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.799\n",
      "INFO:tensorflow:loss = 4.7908106, step = 1201 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.712\n",
      "INFO:tensorflow:loss = 5.560035, step = 1301 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.704\n",
      "INFO:tensorflow:loss = 5.009211, step = 1401 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 357.435\n",
      "INFO:tensorflow:loss = 5.7771587, step = 1501 (0.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.819\n",
      "INFO:tensorflow:loss = 6.2388487, step = 1601 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 406.683\n",
      "INFO:tensorflow:loss = 1.9590427, step = 1701 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 364.679\n",
      "INFO:tensorflow:loss = 4.472778, step = 1801 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.581\n",
      "INFO:tensorflow:loss = 6.326952, step = 1901 (0.267 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.7547796.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f9c8c12f890>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model.\n",
    "classifier.train(input_fn=train_input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-02-08:20:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-02-08:20:51\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.93333334, average_loss = 0.061266568, global_step = 2000, loss = 1.8379971\n",
      "\n",
      "Test Accuracy: 0.933333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(test_set.data)},\n",
    "    y=np.array(test_set.target),\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "\n",
    "print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-2000\n",
      "New Samples, Class Predictions:    [array(['1'], dtype=object), array(['2'], dtype=object)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classify two new flower samples.\n",
    "new_samples = np.array(\n",
    "    [[6.4, 3.2, 4.5, 1.5],\n",
    "     [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": new_samples},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "predictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "predicted_classes = [p[\"classes\"] for p in predictions]\n",
    "\n",
    "print(\n",
    "    \"New Samples, Class Predictions:    {}\\n\"\n",
    "    .format(predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf#导入TensorFlow库\n",
    "\n",
    "#构建数据流图\n",
    "\n",
    "graph = tf.Graph()#显式创建Graph对象\n",
    "with graph.as_default():#设为默认Graph对象\n",
    "    with tf.name_scope(\"variables\"):#创建Variable对象名称作用域\n",
    "        global_step  = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"global_step\")#记录数据流图运行次数的Variable对象，初值为0，数据类型为32位整型，不可自动修改，以global_step标识\n",
    "        total_output = tf.Variable(0.0, dtype=tf.float32, trainable=False, name=\"total_output\")#追踪模型所有输出累加和的Variable对象，初值为0.0，数据类型为32位浮点型，不可自动修改，以total_output标识\n",
    "\n",
    "    with tf.name_scope(\"transformation\"):#创建变换计算Op名称作用域\n",
    "        with tf.name_scope(\"input\"):#创建独立输入层名称作用域\n",
    "            a = tf.placeholder(tf.float32, shape=[None], name=\"input_placeholder_a\")#创建占位符，接收一个32位浮点型任意长度的向量作为输入，以input_placeholder_a标识\n",
    "        \n",
    "        with tf.name_scope(\"intermediate_layer\"):#创建独立中间层名称作用域\n",
    "            b = tf.reduce_prod(a, name=\"product_b\")#创建创建归约乘积Op，接收张量输入，输出张量所有分量(元素)的乘积，以product_b标识\n",
    "            c = tf.reduce_sum(a, name=\"sum_c\")#创建创建归约求和Op，接收张量输入，输出张量所有分量(元素)的求和，以sum_c标识\n",
    "        \n",
    "        with tf.name_scope(\"output\"):#创建独立输出层名称作用域\n",
    "            output = tf.add(b, c, name=\"output\")#创建创建求和Op，接收两个标量输入,输出标量求和,以output标识\n",
    "\n",
    "    with tf.name_scope(\"update\"):\n",
    "        update_total = total_output.assign_add(output)#用最新的输出更新Variable对象total_output\n",
    "        increment_step = global_step.assign_add(1)#增1更新Variable对象global_step，记录数据流图运行次数\n",
    "\n",
    "    with tf.name_scope(\"summaries\"):#创建数据汇总Op名称作用域\n",
    "        avg = tf.div(update_total, tf.cast(increment_step, tf.float32), name=\"average\")#计算平均值，输出累加和除以数据流图运行次数，把运行次数数据类型转换为32位浮点型，以average标识\n",
    "        tf.summary.scalar(b'output_summary',output)#创建输出节点标量数据统计汇总，以output_summary标识\n",
    "        tf.summary.scalar(b'total_summary',update_total)#创建输出累加求和标量数据统计汇总，以total_summary标识\n",
    "        tf.summary.scalar(b'average_summary',avg)#创建平均值标量数据统计汇总，以average_summary标识\n",
    "\n",
    "    with tf.name_scope(\"global_ops\"):#创建全局Operation(Op)名称作用域\n",
    "        init = tf.global_variables_initializer()#创建初始化所有Variable对象的Op\n",
    "        merged_summaries = tf.summary.merge_all()#创建合并所有汇总数据的Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'variables/global_step/initial_value' type=Const>,\n",
       " <tf.Operation 'variables/global_step' type=VariableV2>,\n",
       " <tf.Operation 'variables/global_step/Assign' type=Assign>,\n",
       " <tf.Operation 'variables/global_step/read' type=Identity>,\n",
       " <tf.Operation 'variables/total_output/initial_value' type=Const>,\n",
       " <tf.Operation 'variables/total_output' type=VariableV2>,\n",
       " <tf.Operation 'variables/total_output/Assign' type=Assign>,\n",
       " <tf.Operation 'variables/total_output/read' type=Identity>,\n",
       " <tf.Operation 'transformation/input/input_placeholder_a' type=Placeholder>,\n",
       " <tf.Operation 'transformation/intermediate_layer/Const' type=Const>,\n",
       " <tf.Operation 'transformation/intermediate_layer/product_b' type=Prod>,\n",
       " <tf.Operation 'transformation/intermediate_layer/Const_1' type=Const>,\n",
       " <tf.Operation 'transformation/intermediate_layer/sum_c' type=Sum>,\n",
       " <tf.Operation 'transformation/output/output' type=Add>,\n",
       " <tf.Operation 'update/AssignAdd' type=AssignAdd>,\n",
       " <tf.Operation 'update/AssignAdd_1/value' type=Const>,\n",
       " <tf.Operation 'update/AssignAdd_1' type=AssignAdd>,\n",
       " <tf.Operation 'summaries/Cast' type=Cast>,\n",
       " <tf.Operation 'summaries/average' type=RealDiv>,\n",
       " <tf.Operation 'summaries/output_summary/tags' type=Const>,\n",
       " <tf.Operation 'summaries/output_summary' type=ScalarSummary>,\n",
       " <tf.Operation 'summaries/total_summary/tags' type=Const>,\n",
       " <tf.Operation 'summaries/total_summary' type=ScalarSummary>,\n",
       " <tf.Operation 'summaries/average_summary/tags' type=Const>,\n",
       " <tf.Operation 'summaries/average_summary' type=ScalarSummary>,\n",
       " <tf.Operation 'global_ops/init' type=NoOp>,\n",
       " <tf.Operation 'global_ops/Merge/MergeSummary' type=MergeSummary>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['variables', 'summaries'], ['variables', 'summaries'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_all_collection_keys(), graph.collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'variables/global_step:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'variables/total_output:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_collection(\"variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'summaries/output_summary:0' shape=() dtype=string>,\n",
       " <tf.Tensor 'summaries/total_summary:0' shape=() dtype=string>,\n",
       " <tf.Tensor 'summaries/average_summary:0' shape=() dtype=string>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_collection(\"summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "name_scope() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-db199bdb6dc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/lilinhan/workspace/software/anaconda/envs/tensorflow/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mGeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: name_scope() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "graph.name_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#运行数据流图\n",
    "sess = tf.Session(graph=graph)#用显式创建Graph对象启动Session会话对象\n",
    "writer = tf.summary.FileWriter('./improved_graph', graph)#启动FileWriter对象，保存汇总数据\n",
    "sess.run(init)#运行Variable对象初始化Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_graph(input_tensor):#定义数据注图运行辅助函数\n",
    "    \"\"\"\n",
    "    辅助函数：用给定的输入张量运行数据流图，\n",
    "    并保存汇总数据\n",
    "    \"\"\"\n",
    "    feed_dict = {a: input_tensor}#创建feed_dict参数字典，以input_tensor替换a句柄的tf.placeholder节点值\n",
    "    _, step, summary = sess.run([output, increment_step, merged_summaries], feed_dict=feed_dict)#使用feed_dict运行output不关心存储，运行increment_step保存到step，运行merged_summaries Op保存到summary\n",
    "    writer.add_summary(summary, global_step=step)#添加汇总数据到FileWriter对象，global_step参数时间图示折线图横轴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用不同的输入用例运行数据流图\n",
    "\n",
    "run_graph([2,8])\n",
    "run_graph([3,1,3,3])\n",
    "run_graph([8])\n",
    "run_graph([1,2,3])\n",
    "run_graph([11,4])\n",
    "run_graph([4,1])\n",
    "run_graph([7,3,1])\n",
    "run_graph([6,3])\n",
    "run_graph([0,2])\n",
    "run_graph([4,5,6])\n",
    "writer.flush()#将汇总数据写入磁盘\n",
    "writer.close()#关闭FileWriter对象，释放资源\n",
    "sess.close()#关闭Session对象，释放资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
