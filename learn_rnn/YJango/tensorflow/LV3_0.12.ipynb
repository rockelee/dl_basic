{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FNN(object):\n",
    "    \"\"\"Build a general FeedForward neural network\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float\n",
    "    drop_out : float\n",
    "    Layers : list\n",
    "        The number of layers\n",
    "    N_hidden : list\n",
    "        The numbers of nodes in layers\n",
    "    D_input : int\n",
    "        Input dimension\n",
    "    D_label : int\n",
    "        Label dimension\n",
    "    Task_type : string\n",
    "        'regression' or 'classification'\n",
    "    L2_lambda : float\n",
    "    Auther : YJango; 2016/11/25\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, Layers, N_hidden, D_input, D_label, Task_type='regression', L2_lambda=0.0):\n",
    "        \n",
    "        #var\n",
    "        self.learning_rate = learning_rate\n",
    "        self.Layers = Layers\n",
    "        self.N_hidden = N_hidden\n",
    "        self.D_input = D_input\n",
    "        self.D_label = D_label\n",
    "        # 类型控制loss函数的选择\n",
    "        self.Task_type = Task_type\n",
    "        # l2 regularization的惩罚强弱，过高会使得输出都拉向0\n",
    "        self.L2_lambda = L2_lambda\n",
    "        # 用于存放所累积的每层l2 regularization\n",
    "        self.l2_penalty = tf.constant(0.0)\n",
    "        \n",
    "        # 用于生成tensorflow缩放图的,括号里起名字\n",
    "        with tf.name_scope('Input'):\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, D_input], name=\"inputs\")\n",
    "        with tf.name_scope('Label'):\n",
    "            self.labels = tf.placeholder(tf.float32, [None, D_label], name=\"labels\")\n",
    "        with tf.name_scope('keep_rate'):\n",
    "            self.drop_keep_rate = tf.placeholder(tf.float32, name=\"dropout_keep\")\n",
    "        \n",
    "\n",
    "        # 初始化的时候直接生成，build方法是后面会建立的\n",
    "        self.build('F')\n",
    "        \n",
    "    def weight_init(self,shape):\n",
    "        # shape : list [in_dim, out_dim]\n",
    "        # 在这里更改初始化方法\n",
    "        # 方式1：下面的权重初始化若用ReLU激活函数，可以使用带有6个隐藏层的神经网络\n",
    "        #       若过深，则使用dropout会难以拟合。\n",
    "        #initial = tf.truncated_normal(shape, stddev=0.1)/ np.sqrt(shape[1])\n",
    "        # 方式2：下面的权重初始化若用ReLU激活函数，可以扩展到15个隐藏层以上（通常不会用那么多）\n",
    "        initial = tf.random_uniform(shape,minval=-np.sqrt(5)*np.sqrt(1.0/shape[0]), maxval=np.sqrt(5)*np.sqrt(1.0/shape[0]))\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_init(self,shape):\n",
    "        # can change initialization here\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def variable_summaries(self, var, name):\n",
    "        with tf.name_scope(name+'_summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.histogram('mean/' + name, mean)\n",
    "        with tf.name_scope(name+'_stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        # 记录每次训练后变量的数值变化\n",
    "        tf.summary.scalar('_stddev/' + name, stddev)\n",
    "        tf.summary.scalar('_max/' + name, tf.reduce_max(var))\n",
    "        tf.summary.scalar('_min/' + name, tf.reduce_min(var))\n",
    "        tf.summary.histogram(name, var)\n",
    "\n",
    "    def layer(self,in_tensor, in_dim, out_dim, layer_name, act=tf.nn.relu):\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope(layer_name+'_weights'):\n",
    "                # 用所建立的weight_init函数进行初始化。\n",
    "                weights = self.weight_init([in_dim, out_dim])\n",
    "                # 存放着每一个权重W\n",
    "                self.W.append(weights)\n",
    "                # 对权重进行统计\n",
    "                self.variable_summaries(weights, layer_name + '/weights')\n",
    "            with tf.name_scope(layer_name+'_biases'):\n",
    "                biases = self.bias_init([out_dim])\n",
    "                # 存放着每一个偏移b\n",
    "                self.b.append(biases)\n",
    "                self.variable_summaries(biases, layer_name + '/biases')\n",
    "            with tf.name_scope(layer_name+'_Wx_plus_b'):\n",
    "                # 计算Wx+b\n",
    "                pre_activate = tf.matmul(in_tensor, weights) + biases\n",
    "                # 记录直方图\n",
    "                tf.summary.histogram(layer_name + '/pre_activations', pre_activate)\n",
    "            # 计算a(Wx+b)\n",
    "            activations = act(pre_activate, name='activation')\n",
    "            tf.summary.histogram(layer_name + '/activations', activations)\n",
    "        # 最终返回该层的输出，以及权重W的L2\n",
    "        return activations, tf.nn.l2_loss(weights)\n",
    "\n",
    "    def drop_layer(self,in_tensor):\n",
    "            #tf.scalar_summary('dropout_keep', self.drop_keep_rate)\n",
    "        dropped = tf.nn.dropout(in_tensor, self.drop_keep_rate)\n",
    "        return dropped\n",
    "\n",
    "    def build(self, prefix):\n",
    "        # 建立网络 \n",
    "        # incoming也代表当前tensor的流动位置\n",
    "        incoming = self.inputs\n",
    "        # 如果没有隐藏层\n",
    "        if self.Layers!=0:\n",
    "            layer_nodes = [self.D_input] + self.N_hidden\n",
    "        else:\n",
    "            layer_nodes = [self.D_input]\n",
    "        \n",
    "        # hid_layers用于存储所有隐藏层的输出\n",
    "        self.hid_layers=[]\n",
    "        # W用于存储所有层的权重\n",
    "        self.W=[]\n",
    "        # b用于存储所有层的偏移\n",
    "        self.b=[]\n",
    "        # total_l2用于存储所有层的L2\n",
    "        self.total_l2=[]\n",
    "        \n",
    "        # 开始叠加隐藏层。这跟千层饼没什么区别。\n",
    "        for l in range(self.Layers):\n",
    "            # 使用刚才编写的函数来建立层，并更新incoming的位置\n",
    "            if l+2 >len(layer_nodes):\n",
    "                nodes_in=layer_nodes[-1]\n",
    "                nodes_out=layer_nodes[-1]\n",
    "            else:\n",
    "                nodes_in=layer_nodes[l]\n",
    "                nodes_out=layer_nodes[l+1]\n",
    "            incoming,l2_loss= self.layer(incoming,nodes_in,nodes_out,prefix+'_hid_'+str(l+1),act=tf.nn.relu)\n",
    "            # 累计l2\n",
    "            self.total_l2.append(l2_loss)\n",
    "            # 输出一些信息，让我们知道网络在建造中做了什么\n",
    "            print('Add dense layer: relu')\n",
    "            print('    %sD --> %sD' %(nodes_in,nodes_out))\n",
    "            # 存储所有隐藏层的输出\n",
    "            self.hid_layers.append(incoming)\n",
    "            # 加入dropout层\n",
    "            incoming = self.drop_layer(incoming)\n",
    "            \n",
    "        # 输出层的建立。输出层需要特别对待的原因是输出层的activation function要根据任务来变。\n",
    "        # 回归任务的话，下面用的是tf.identity，也就是没有activation function\n",
    "        if self.Task_type=='regression':\n",
    "            out_act=tf.identity\n",
    "        else:\n",
    "            # 分类任务使用softmax来拟合概率\n",
    "            out_act=tf.nn.softmax\n",
    "        self.output,l2_loss= self.layer(incoming,layer_nodes[-1],self.D_label, layer_name='output',act=out_act)\n",
    "        self.total_l2.append(l2_loss)\n",
    "        print('Add output layer: linear')\n",
    "        print('    %sD --> %sD' %(layer_nodes[-1],self.D_label))\n",
    "        \n",
    "        # l2 loss的缩放图\n",
    "        with tf.name_scope('total_l2'):\n",
    "            for l2 in self.total_l2:\n",
    "                self.l2_penalty+=l2\n",
    "            tf.summary.histogram('l2_penalty', self.l2_penalty)\n",
    "            \n",
    "        # 不同任务的loss\n",
    "        # 若为回归，则loss是用于判断所有预测值和实际值差别的函数。\n",
    "        if self.Task_type=='regression':\n",
    "            with tf.name_scope('SSE'):\n",
    "                self.loss=tf.reduce_mean((self.output-self.labels)**2)\n",
    "                self.loss2=tf.nn.l2_loss(self.output-self.labels)\n",
    "                \n",
    "                tf.summary.histogram('loss', self.loss)\n",
    "        else:\n",
    "            # 若为分类，cross entropy的loss function\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(self.output, self.labels)\n",
    "            with tf.name_scope('cross entropy'):\n",
    "                self.loss = tf.reduce_mean(entropy)\n",
    "                tf.summary.histogram('loss', self.loss)\n",
    "            with tf.name_scope('accuracy'):\n",
    "                correct_prediction = tf.equal(tf.argmax(self.output, 1), tf.argmax(self.labels, 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                tf.summary.histogram('accuracy', self.accuracy)\n",
    "                \n",
    "        # 整合所有loss，形成最终loss\n",
    "        with tf.name_scope('total_loss'):\n",
    "            self.total_loss=self.loss + self.l2_penalty*self.L2_lambda\n",
    "            tf.summary.histogram('total_loss', self.total_loss)\n",
    "            \n",
    "        # 训练操作\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss)\n",
    "\n",
    "        # 洗牌功能\n",
    "    def shufflelists(self,lists):\n",
    "        ri=np.random.permutation(len(lists[1]))\n",
    "        out=[]\n",
    "        for l in lists:\n",
    "            out.append(l[ri])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Standardize(seq):\n",
    "    #subtract mean\n",
    "    centerized=seq-np.mean(seq, axis = 0)\n",
    "    #divide standard deviation\n",
    "    normalized=centerized/np.std(centerized, axis = 0)\n",
    "    return normalized\n",
    "def Makewindows(indata,window_size=41):\n",
    "    outdata=[]\n",
    "    mid=int(window_size/2)\n",
    "    indata=np.vstack((np.zeros((mid,indata.shape[1])),indata,np.zeros((mid,indata.shape[1]))))\n",
    "    for i in range(indata.shape[0]-window_size+1):\n",
    "        outdata.append(np.hstack(indata[i:i+window_size]))\n",
    "    return np.array(outdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfc=np.load('X.npy')\n",
    "art=np.load('Y.npy')\n",
    "x=[]\n",
    "y=[]\n",
    "for i in range(len(mfc)):\n",
    "    x.append(Makewindows(Standardize(mfc[i])))\n",
    "    y.append(Standardize(art[i]))\n",
    "vali_size=20\n",
    "totalsamples=len(np.vstack(x))\n",
    "X_train=np.vstack(x)[int(totalsamples/vali_size):].astype(\"float32\")\n",
    "Y_train=np.vstack(y)[int(totalsamples/vali_size):].astype(\"float32\")\n",
    "\n",
    "X_test=np.vstack(x)[:int(totalsamples/vali_size)].astype(\"float32\")\n",
    "Y_test=np.vstack(y)[:int(totalsamples/vali_size)].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((37500, 1599), (37500, 24), (1973, 1599), (1973, 24))\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add dense layer: relu\n",
      "    1599D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add dense layer: relu\n",
      "    1024D --> 1024D\n",
      "Add output layer: linear\n",
      "    1024D --> 24D\n"
     ]
    }
   ],
   "source": [
    "# 生成网络实例\n",
    "# 如果不指定全部隐藏层节点，则多出的隐藏层全部按最后一个指定的隐藏层节点数设定\n",
    "ff=FNN(learning_rate=7e-5, Layers=50,N_hidden=[1024], D_input=1599, D_label=24, L2_lambda=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('log' + '/train',sess.graph)\n",
    "test_writer = tf.summary.FileWriter('log' + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plots(T,P,i, n=21,length=400):\n",
    "    m=0\n",
    "    plt.figure(figsize=(20,16))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(T[m:m+length,7],'--')\n",
    "    plt.plot(P[m:m+length,7])\n",
    "\n",
    "    plt.subplot(412)\n",
    "    plt.plot(T[m:m+length,8],'--')\n",
    "    plt.plot(P[m:m+length,8])\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.plot(T[m:m+length,15],'--')\n",
    "    plt.plot(P[m:m+length,15])\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.plot(T[m:m+length,16],'--')\n",
    "    plt.plot(P[m:m+length,16])\n",
    "    plt.legend(['True','Predicted'])\n",
    "    plt.savefig('epoch'+str(i)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 | train_loss:[0.60085243] |test_loss:0.659336\n",
      "epoch1 | train_loss:[0.56255817] |test_loss:0.653993\n",
      "epoch2 | train_loss:[0.53966987] |test_loss:0.644001\n",
      "epoch3 | train_loss:[0.47888839] |test_loss:0.609307\n",
      "epoch4 | train_loss:[0.4274255] |test_loss:0.580894\n",
      "epoch5 | train_loss:[0.41663638] |test_loss:0.58645\n",
      "epoch6 | train_loss:[0.39932683] |test_loss:0.594072\n",
      "epoch7 | train_loss:[0.35524291] |test_loss:0.57222\n",
      "epoch8 | train_loss:[0.35336751] |test_loss:0.565518\n",
      "epoch9 | train_loss:[0.30439419] |test_loss:0.545433\n"
     ]
    }
   ],
   "source": [
    "# 训练并记录\n",
    "k=0\n",
    "EPOCH=10\n",
    "Batch=256\n",
    "for i in range(EPOCH):\n",
    "    idx=0\n",
    "    X0,Y0=ff.shufflelists([X_train,Y_train])\n",
    "    while idx<X_train.shape[0]:\n",
    "        sess.run(ff.train_step,feed_dict={ff.inputs:X0[idx:idx+Batch],ff.labels:Y0[idx:idx+Batch],ff.drop_keep_rate:1.0})#当keep rate设为1.0时，表示不运用dropout\n",
    "        idx+=Batch\n",
    "        k+=1\n",
    "    #可以选择隔一段时间一记录\n",
    "    pL_train=sess.run([ff.loss],feed_dict={ff.inputs:X_train,ff.labels:Y_train,ff.drop_keep_rate:1.0})\n",
    "    #train_writer.add_summary(summary, k)\n",
    "    pY,pL_test=sess.run([ff.output,ff.loss],feed_dict={ff.inputs:X_test,ff.labels:Y_test,ff.drop_keep_rate:1.0})\n",
    "    plots(Y_test,pY,i)\n",
    "    #test_writer.add_summary(summary, k)\n",
    "    print('epoch%s | train_loss:%s |test_loss:%s' %(i,pL_train,pL_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用完关闭session\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
